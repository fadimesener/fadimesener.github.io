<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Fadime Sener</title>

  <meta name="author" content="Fadime Sener">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" type="image/png" href="images/Blue_Jays.png">
  <title>Fadime Sener</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>

  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-113195086-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-113195086-1');
</script>

<style type="text/css">
    /* Color scheme stolen from Sergey Karayev */

    a {
      color: #2d59eb;
      text-decoration: none;
    }
    b {
      color: #000000;
      text-decoration: none;
    }
    c {
      color: #0BDA51;
      text-decoration: none;
    }
    d {
      color: #FFA500;
      text-decoration: none;
    }
    a:focus,
    a:hover {
      color: #af19bf;
      text-decoration: none;
    }

    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 16px
      background-image: "images/jose/bg.png";
       /*background-color: #fffff;*/
    }

    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 16px;
    }

    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 24px;
    }

    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14.5px;
      font-weight: 600
    }

    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 34px;
    }

    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }

    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }

    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }

    .news-scroll-box {
  height: 300px;
  overflow: auto;
  background-color: #f7f5f5;
  padding: 2px 10px 10px 10px;
}

    span.highlight {
      background-color: #CA7FD4;
    }
  </style>


</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Fadime Sener</name>
              </p>
              <p style="text-align:justify">
              I‚Äôm  Fadime Sener,
              </p>
              <p style="text-align:justify">
              My research focuses on image and multi-modal video recognition.
              </p>
              </p>
              <p style="text-align:center">
                <a href="mailto:sener@cs.uni-bonn.de">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=en&user=-juoweoAAAAJ&view_op=list_works&sortby=pubdate">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/fadime-sener-674064156/">LinkedIn</a>
              </p>
            </td>
            <td style="padding:0.75%;width:35%;max-width:35%">
              <a href="data/fs.jpg"><img style="width:90%;max-width:90%" alt="profile photo" src="data/fs.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>




<!--News---------------------------------------------->
  <table id="news" class="table" align="right" border="0" cellspacing="0" cellpadding="10">
    <tr class="row">
      <td class="cell">
        <a id="news"><heading>News</heading></a>
        <div class="news-scroll-box">
          <ul>
            <li>
              <strong>[July 2024]</strong>
              <a href="https://s-shamil.github.io/HandFormer/">HandFormer</a>
              has been accepted to <a href="https://eccv2024.ecva.net/">ECCV 2024</a>.
            </li>
            <li>
              <strong>[June 2024]</strong>
              <a href=" https://assembly-101.github.io/">Assembly101</a>
              was awarded the
              <a href="https://egovis.github.io/awards/2022_2023/">2022/2023 Distinguished Paper Award</a>
              at the
              <a href="https://egovis.github.io/cvpr24/">EgoVis</a>
              workshop.
            </li>
            <li>
              <strong>[September 2023]</strong> "<a href="https://dibschat.github.io/openvocab-egoAR/">Opening the
                Vocabulary of Egocentric Actions</a>" has been accepted to
              <a href="https://nips.cc/">NeurIPS 2023</a>.
            </li>
            <li>
              <strong>[October 2022]</strong> Organized
              <a href="https://sites.google.com/view/egocentric-hand-body-activity">Human Body, Hands, and Activities
                from Egocentric and
                Multi-view Cameras</a>
              (HBHA) workshop at ECCV 2022.
            </li>
            <li>
              <strong>[March 2022]</strong> Paper proposing a new
              procedural activity dataset
              <a href="https://assembly-101.github.io/">Assembly101</a>
              has been accepted to CVPR 2022.
            </li>
            <li>
              06/21: &nbsp &nbsp   Defended my PhD! üéâ üë©‚Äçüéì
            </li>

          </ul>
        </div>
      </td>
    </tr>
  </table>



<!--Publication  Start ---------------------------------------------->
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
<tbody>
<tr>
<td style="padding:20px;width:100%;vertical-align:left"> <br><br>
<a id="news"><heading>Publications</heading></a>
</td>
</tr>
</tbody>
</table>
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
<tbody>


  <!-- ITEM -->
  <tr>
    <td style="padding:5px;width:40%;vertical-align:middle">
        <div style="text-align: center">
        <img src="images/tailTAS.jpg" alt="longtailTAS" width="250" height="120" class="center">
        </div>
    </td>
    <td width="75%" valign="middle"><br>
        <a href="https://arxiv.org/pdf/2403.09805">
        <papertitle>Long-Tail Temporal Action Segmentation with Group-wise Temporal Logit Adjustment</papertitle>
        </a> <br>
        <a href="https://pangzhan27.github.io/">Zhanzhong Pang</a>,
        <b>Fadime Sener*</b>,
        <a href="https://stablegradients.github.io/">Shrinivas Ramasubramanian</a>,
        <a href="https://www.comp.nus.edu.sg/~ayao/">Angela Yao</a> <br>
      <em>ECCV, 2024</em>
      <p>
        <a href="https://arxiv.org/pdf/2408.09919">Paper</a>  |
        <a href="https://github.com/pangzhan27/GTLA">Code</a>   <br><br>
    </td>
    </tr>

<!-- ITEM -->
  <tr>
    <td style="padding:5px;width:40%;vertical-align:middle">
        <div style="text-align: center">
        <img src="images/handformer.png" alt="handformer" width="250" height="120" class="center">
        </div>
    </td>
    <td width="75%" valign="middle"><br>
        <a href="https://arxiv.org/pdf/2403.09805">
        <papertitle>On the Utility of 3D Hand Poses for Action Recognition</papertitle>
        </a> <br>
        <a href="https://www.comp.nus.edu.sg/~salman/">Md Salman Shamil</a>,
        <a href="https://dibschat.github.io/">Dibyadip Chatterjee</a>,
        <b>Fadime Sener*</b>,
        <a href="https://shugaoma.github.io/">Shugao Ma</a>,
        <a href="https://www.comp.nus.edu.sg/~ayao/">Angela Yao</a> <br>
      <em>ECCV, 2024</em>
      <p>
        <a href="https://arxiv.org/pdf/2403.09805">Paper</a>  |
        <a href="https://github.com/s-shamil/HandFormer">Code</a> |
        <a href="https://s-shamil.github.io/HandFormer/">Project</a> <br><br>
    </td>
    </tr>


    <!-- ITEM -->
  <tr>
    <td style="padding:5px;width:40%;vertical-align:middle">
        <div style="text-align: center">
        <img src="images/diffh2o.jpg" alt="DiffH2O" width="220" height="150" class="center">
        </div>
    </td>
    <td width="75%" valign="middle"><br>
        <a href="https://arxiv.org/pdf/2403.17827">
        <papertitle>DiffH2O: Diffusion-Based Synthesis of Hand-Object Interactions from Textual Descriptions</papertitle>
        </a> <br>
        <a href="https://ait.ethz.ch/people/sammyc">Sammy Christen</a>,
        <a href="https://shreyashampali.github.io/">Shreyas Hampali</a>,
        <b>Fadime Sener*</b>,
        <a href="https://scholar.google.com/citations?user=yz2P_aUAAAAJ&hl=en">Edoardo Remelli</a>,
        <a href="https://cmp.felk.cvut.cz/~hodanto2/">Tomas Hodan</a>,
        <a href="https://scholar.google.com/citations?user=HDYUYvwAAAAJ&hl=en">Eric Sauser </a>,
        <a href="https://shugaoma.github.io/">Shugao Ma</a><br>
        <a href="https://btekin.github.io/">Bugra Tekin</a>,
      <em>SIGGRAPH Asia, 2024</em>
      <p>
        <a href="https://arxiv.org/pdf/2403.17827">Paper</a>  |
        <a href="https://diffh2o.github.io/">Project</a> <br><br>
    </td>
    </tr>


<!-- ITEM -->
  <tr>
  <td style="padding:5px;width:40%;vertical-align:middle">
      <div style="text-align: center">
      <img src="images/xmic.png" alt="XMIC" width="220" height="150" class="center">
      </div>
  </td>
  <td width="75%" valign="middle"><br>
      <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Kukleva_X-MIC_Cross-Modal_Instance_Conditioning_for_Egocentric_Action_Generalization_CVPR_2024_paper.pdf">
      <papertitle>X-MIC: Cross-Modal Instance Conditioning for Egocentric Action Generalization</papertitle>
      </a> <br>
      <a href="https://annusha.github.io/">Anna Kukleva</a>,
      <b>Fadime Sener*</b>,
      <a href="https://scholar.google.com/citations?user=yz2P_aUAAAAJ&hl=en">Edoardo Remelli</a>,
      <a href="https://btekin.github.io/">Bugra Tekin</a>,
      <a href="https://scholar.google.com/citations?user=HDYUYvwAAAAJ&hl=en">Eric Sauser </a>,
      <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/people/bernt-schiele">Bernt Schiele</a>,
      <a href="https://shugaoma.github.io/">Shugao Ma</a><br>
    <em>CVPR, 2024</em>
    <p>
      <a href="https://arxiv.org/pdf/2403.19811v1.pdf">Paper</a>  |
      <a href="https://github.com/Annusha/xmic">Code</a> <br><br>
  </td>
  </tr>


<!-- ITEM -->
  <tr>
    <td style="padding:5px;width:40%;vertical-align:middle">
        <div style="text-align: center">
        <img src="images/openegoAR.png" alt="openegoAR" width="250" height="120" class="center">
        </div>
    </td>
    <td width="75%" valign="middle"><br>
        <a href="https://arxiv.org/pdf/2308.11488">
        <papertitle>Opening the Vocabulary of Egocentric Actions </papertitle>
        </a> <br>
        <a href="https://dibschat.github.io/">Dibyadip Chatterjee</a>,
        <b>Fadime Sener*</b>,
        <a href="https://shugaoma.github.io/">Shugao Ma</a>,
        <a href="https://www.comp.nus.edu.sg/~ayao/">Angela Yao</a> <br>
      <em>NeurIPS, 2023</em>
      <p>
        <a href="https://arxiv.org/pdf/2308.11488">Paper</a>  |
        <a href="https://github.com/dibschat/openvocab-egoAR">Code</a> |
        <a href="https://dibschat.github.io/openvocab-egoAR/">Project</a> <br><br>
    </td>
    </tr>


<!-- ITEM -->
<tr>
  <td style="padding:5px;width:40%;vertical-align:middle">
      <div style="text-align: center">
      <img src="images/taskTAS.png" alt="openegoAR" width="250" height="120" class="center">
      </div>
  </td>
  <td width="75%" valign="middle"><br>
      <a href="https://arxiv.org/pdf/2210.10352">
      <papertitle>Temporal Action Segmentation: An Analysis of Modern Techniques </papertitle>
      </a> <br>
      <a href="https://guodongding.cn/">Guodong Ding</a>,
      <b>Fadime Sener*</b>,
      <a href="https://www.comp.nus.edu.sg/~ayao/">Angela Yao</a> <br>
    <em>TPAMI, 2023</em>
    <p>
      <a href="https://arxiv.org/pdf/2210.10352">Paper</a>  |
      <a href="https://github.com/nus-cvml/awesome-temporal-action-segmentation">Project</a> <br><br>
  </td>
  </tr>

  <!-- ITEM -->
<tr>
  <td style="padding:5px;width:40%;vertical-align:middle">
      <div style="text-align: center">
      <img src="images/mistake.jpg" alt="openegoAR" width="250" height="120" class="center">
      </div>
  </td>
  <td width="75%" valign="middle"><br>
      <a href="https://openreview.net/pdf?id=SNzpTSuuVJ">
      <papertitle>Every Mistake Counts: Spatial and Temporal Beliefs for Mistake Detection in Assembly Tasks </papertitle>
      </a> <br>
      <a href="https://guodongding.cn/">Guodong Ding</a>,
      <b>Fadime Sener*</b>,
      <a href="https://shugaoma.github.io/">Shugao Ma</a>,
      <a href="https://www.comp.nus.edu.sg/~ayao/">Angela Yao</a> <br>
    <em> 2023</em>
    <p>
      <a href=https://openreview.net/pdf?id=SNzpTSuuVJ">Paper</a>  <br><br>
  </td>
  </tr>



<!-- ITEM -->
<tr>
  <td style="padding:5px;width:40%;vertical-align:middle">
      <div style="text-align: center">
      <img src="images/assemblyhands.png" alt="openegoAR" width="210" height="160" class="center">
      </div>
  </td>
  <td width="75%" valign="middle"><br>
      <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ohkawa_AssemblyHands_Towards_Egocentric_Activity_Understanding_via_3D_Hand_Pose_Estimation_CVPR_2023_paper.pdf">
      <papertitle>AssemblyHands: Towards Egocentric Activity Understanding via 3D Hand Pose Estimation </papertitle>
      </a> <br>
      <a href="https://dibschat.github.io/">Dibyadip Chatterjee</a>,
      <b>Fadime Sener*</b>,
      <a href="https://shugaoma.github.io/">Shugao Ma</a>,
      <a href="https://www.comp.nus.edu.sg/~ayao/">Angela Yao</a> <br>
    <em>CVPR, 2023</em>
    <p>
      <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ohkawa_AssemblyHands_Towards_Egocentric_Activity_Understanding_via_3D_Hand_Pose_Estimation_CVPR_2023_paper.pdf">Paper</a>  |
      <a href="https://github.com/facebookresearch/assemblyhands-toolkit">Code</a> |
      <a href="https://assemblyhands.github.io/">Project</a> <br><br>
  </td>
  </tr>


<!-- ITEM -->
  <tr>
    <td style="padding:5px;width:40%;vertical-align:middle">
        <div style="text-align: center">
        <img src="images/assembly.png" alt="openegoAR" width="250" height="120" class="center">
        </div>
    </td>
    <td width="75%" valign="middle"><br>
        <a href="https://arxiv.org/pdf/2308.11488">
        <papertitle>Assembly101: A Large-Scale Multi-View Video Dataset for Understanding Procedural Activities </papertitle>
        </a> <br>
        <b>Fadime Sener*</b>,
        <a href="https://dibschat.github.io/">Dibyadip Chatterjee</a>,
        <a href="">Daniel Shelepov</a>,
        <a href="">Kun He</a>,
        <a href="">Dipika Singhania</a>,
        <a href="">Robert Wang</a>,
        <a href="https://www.comp.nus.edu.sg/~ayao/">Angela Yao</a> <br>
      <em>CVPR, 2022</em><br>
      <em> <a href="https://egovis.github.io/awards/2022_2023/"> EgoVis 2022/2023 Distinguished Paper Winner</em>
      <p>
        <a href="https://arxiv.org/pdf/2308.11488">Paper</a>  |
        <a href="https://github.com/assembly-101?tab=repositories">Code</a> |
        <a href="https://assembly-101.github.io/">Project</a> |
        <a href="https://www.youtube.com/watch?v=8T8x-iWa3qw">Video</a> <br><br>
    </td>
    </tr>


<!-- ITEM -->
  <tr>
  <td style="padding:5px;width:40%;vertical-align:middle">
    <div style="text-align: center">
    <img src="images/cvpr19.png" alt="cet" width="240" height="100" class="center">
    </div>
  </td>
  <td width="75%" valign="middle"><br>
    <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Kukleva_Unsupervised_Learning_of_Action_Classes_With_Continuous_Temporal_Embedding_CVPR_2019_paper.pdf">
    <papertitle>Unsupervised Learning of Action Classes with Continuous Temporal Embedding</papertitle>
    </a><br>
        <a href="https://annusha.github.io/">Anna Kukleva</a>,
        <a href="https://hildekuehne.github.io/">Hilde Kuehne</a>,
        <b>Fadime Sener*</b>,
        <a href="https://pages.iai.uni-bonn.de/gall_juergen/">J√ºrgen Gall</a> <br>
      <em>CVPR, 2019</em>
      <p>
        <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Kukleva_Unsupervised_Learning_of_Action_Classes_With_Continuous_Temporal_Embedding_CVPR_2019_paper.pdf">Paper</a> |
        <a href="https://github.com/Annusha/unsup_temp_embed">Code</a> <br><br>


  </td>
  </tr>

</tbody>
</table>
<!--Publication  End ---------------------------------------------->


<!--Ref Start ---------------------------------------------->
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
<tbody>
  <tr>
  <td style="padding:0px"> <br>
  <p style="text-align:right;font-size:small;"> <br>
  Stolen from <a href="https://jonbarron.info/">Jon Barron</a>. Big thanks!
  </p>
  </td>
  </tr>
</tbody>
</table>
<!--Ref End ---------------------------------------------->


</td>
</tr>
</table>
</body>
</html>
