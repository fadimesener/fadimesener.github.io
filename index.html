<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Fadime Sener</title>

  <meta name="author" content="Fadime Sener">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" type="image/png" href="images/Blue_Jays.png">
  <title>Fadime Sener</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>

  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-113195086-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-113195086-1');
</script>

<style type="text/css">
    /* Color scheme stolen from Sergey Karayev */

    a {
      color: #2d59eb;
      text-decoration: none;
    }
    b {
      color: #000000;
      text-decoration: none;
    }
    c {
      color: #0BDA51;
      text-decoration: none;
    }
    d {
      color: #FFA500;
      text-decoration: none;
    }
    a:focus,
    a:hover {
      color: #af19bf;
      text-decoration: none;
    }

    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 16px
      background-image: "images/jose/bg.png";
       /*background-color: #fffff;*/
    }

    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 16px;
    }

    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 24px;
    }

    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14.5px;
      font-weight: 600
    }

    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 34px;
    }

    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }

    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }

    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }

    .news-scroll-box {
  height: 300px;
  overflow: auto;
  background-color: #f7f5f5;
  padding: 2px 10px 10px 10px;
}

    span.highlight {
      background-color: #CA7FD4;
    }
  </style>


</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Fadime Sener</name>
              </p>
              <p style="text-align:justify">
                I am a Research Scientist at Meta Reality Labs.
                Before this, I spent around seven months at the National University of Singapore (NUS) and four years at the University of Bonn as a research assistant working with Angela Yao and Juergen Gall.
                I received my PhD from the University of Bonn under the supervision of Prof. Angela Yao.
                Before that, I completed my Master‚Äôs degree in Computer Science at Bilkent University and my Bachelor‚Äôs degree in Computer Science at Hacettepe University.
              </p>
              <p style="text-align:justify">
               My research focuses on computer vision and natural language processing. During my PhD, I worked on video understanding through vision and language. Recently, I have been working on online action recognition and leveraging multimodal large language models for streaming video understanding.
              </p>
              </p>
              <p style="text-align:center">
                <a href="mailto:sener@cs.uni-bonn.de">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=en&user=-juoweoAAAAJ&view_op=list_works&sortby=pubdate">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/fadime-sener-674064156/">LinkedIn</a>
              </p>
            </td>
            <td style="padding:0.75%;width:35%;max-width:35%">
              <a href="data/fs.jpg"><img style="width:90%;max-width:90%" alt="profile photo" src="data/fs.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>




<!--News---------------------------------------------->
<table id="news" class="table" align="right" border="0" cellspacing="0" cellpadding="10">
<tr class="row">
<td class="cell">
<a id="news"><heading>News</heading></a>
<div class="news-scroll-box">
<ul>
 

<!-- ITEM -->
<li>
  06/25: &nbsp;&nbsp;
   <a href="https://dibschat.github.io/ProVideLLM/">ProVideLLM</a> ranked 2nd in the 
  <a href="https://eval.ai/web/challenges/challenge-page/2273/overview">EgoExo4D Fine-grained Keystep Recognition Challenge</a> 
  at CVPR 2025.
</li>


<!-- ITEM -->
<li>
  06/25: &nbsp &nbspCo-Organized
  <a href="https://varworkshop.github.io/">
    Vision-based Assistants in the Real-World  </a>
    workshop at CVPR 2025.
</li>

<!-- ITEM -->
<li>
  06/25: &nbsp &nbsp
  <a href="https://assemblyhands.github.io/">AssemblyHands</a>
  was awarded the
  <a href="https://egovis.github.io/awards/2023_2024//">2023/2024 Distinguished Paper Award</a>
  at the
  <a href="https://egovis.github.io/cvpr25/">EgoVis</a>
  workshop.
</li>

<!-- ITEM -->
<li>
  06/24: &nbsp &nbsp
  <a href="https://assembly-101.github.io/">Assembly101</a>
  was awarded the
  <a href="https://egovis.github.io/awards/2022_2023/">2022/2023 Distinguished Paper Award</a>
  at the
  <a href="https://egovis.github.io/cvpr24/">EgoVis</a>
  workshop.
</li>

<!-- ITEM -->
<li>
  10/22: &nbsp &nbspCo-Organized
  <a href="https://nus-cvml.github.io/atlas-eccv22/">
    ATLAS: AcTion Localization And Segmentation in Video  </a>
    tutorial at ECCV 2022.
</li>

<!-- ITEM -->
<li>
  10/22: &nbsp &nbspCo-Organized
  <a href="https://sites.google.com/view/egocentric-hand-body-activity">
    Human Body, Hands, and Activities from Egocentric and  Multi-view Cameras</a>
    (HBHA) workshop at ECCV 2022.
</li>

<!-- ITEM -->
<li>
  03/22: &nbsp &nbsp   Released
  <a href="https://assembly-101.github.io/">Assembly101</a>
   at CVPR 2022.
</li>

<!-- ITEM -->
<li>
  07/21: &nbsp &nbsp   Defended my PhD! üéâ üë©‚Äçüéì
</li>

<!-- ITEM -->
<li>
  06/21: &nbsp &nbsp   Outstanding reviewer at CVPR 2021
</li>

<!-- ITEM -->
<li>
  10/20: &nbsp &nbsp   Started working at Meta Reality Labs as Research Scientist
</li>

<!-- ITEM -->
<li>
  06/20: &nbsp &nbsp   Ranked first in the action anticipation and second in the action
  recognition categories of the EPIC-KITCHENS Dataset - 2020 Challenge with
  <a href="hhttps://arxiv.org/pdf/2006.00830">
    Temporal aggregate representations for long term video understanding</a>
</li>

<!-- ITEM -->
<li>
  01/20: &nbsp &nbsp   Started working as Research Assistant at the
  <a href="https://www.comp.nus.edu.sg/">
    National University of Singapore: NUS</a>
</li>

<!-- ITEM -->
<li>
  07/16: &nbsp &nbsp   Attended International Computer Vision Summer School (ICVSS) 2016
</li>

<!-- ITEM -->
<li>
  10/15: &nbsp &nbsp   Started working as Research Assistant at the
  <a href="https://www.uni-bonn.de/en/studying/degree-programs/degree-programs-a-z/computer-science-msc/">
    University of Bonn</a>
</li>

<!-- ITEM -->
<li>
  06/13: &nbsp &nbsp   Best poster award at  INRIA CVML Summer School 2013 with
  <a href="https://web.cs.hacettepe.edu.tr/~nazli/publications/cm_ap_workshop.pdf">
    On recognizing actions in still images via multiple features</a>
</li>

</ul>
</div>
</td>
</tr>
</table>



<!--Publication  Start ---------------------------------------------->
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
<tbody>
<tr>
<td style="padding:20px;width:100%;vertical-align:left"> <br><br>
<a id="news"><heading>Publications</heading></a>
</td>
</tr>
</tbody>
</table>
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
<tbody>




<!-- ITEM -->
<tr>
    <td style="padding:5px;width:40%;vertical-align:middle">
        <div style="text-align: center">
        <img src="images/ProVideoLLM.jpg" alt="openegoAR" width="250" height="70" class="center">
        </div>
    </td>
    <td width="75%" valign="middle"><br>
        <a href="https://arxiv.org/pdf/2504.13915">
        <papertitle>Memory-efficient Streaming VideoLLMs for Real-time Procedural Video Understanding </papertitle>
        </a> <br>
        <a href="https://dibschat.github.io/">Dibyadip Chatterjee</a>,
        <a href="https://scholar.google.com/citations?user=yz2P_aUAAAAJ&hl=en">Edoardo Remelli</a>,
        <a href="https://people.csail.mit.edu/yalesong/home/">Yale Song</a>,
        <a href="https://btekin.github.io/">Bugra Tekin</a>,
        <a href="https://scholar.google.com/citations?user=BwE_L4MAAAAJ&hl=en">Abhay Mittal</a>,
        <a href="https://scholar.google.co.in/citations?user=M_NWm1wAAAAJ&hl=en">Bharat Lal Bhatnagar</a>,
        <a href="https://www.cihancamgoz.com/">Necati Cihan Camgoz</a>,
        <a href="https://shreyashampali.github.io/">Shreyas Hampali</a>,
        <a href="https://scholar.google.com/citations?user=HDYUYvwAAAAJ&hl=en">Eric Sauser</a>,
        <a href="https://scholar.google.com/citations?user=SUd2LJUAAAAJ&hl=en">Shugao Ma</a>, 
        <a href="https://www.comp.nus.edu.sg/~ayao/">Angela Yao</a> <br>
        <b>Fadime Sener*</b>   
      <em> Arxiv, 2025</em> 
      <p>
        <a href="https://arxiv.org/pdf/2504.13915">Paper</a>  |
        <a href="https://dibschat.github.io/ProVideLLM/">Project</a> <br><br>
    </td>
</tr>


<!-- ITEM -->
<tr>
    <td style="padding:5px;width:40%;vertical-align:middle">
        <div style="text-align: center">
        <img src="images/CMeRT.jpg" alt="openegoAR" width="250" height="50" class="center">
        </div>
    </td>
    <td width="75%" valign="middle"><br>
        <a href="https://arxiv.org/pdf/2503.18359?">
        <papertitle>Context-Enhanced Memory-Refined Transformer for Online Action Detection </papertitle>
        </a> <br>
        <a href="https://pangzhan27.github.io/">Zhanzhong Pang</a>,
        <b>Fadime Sener*</b>, 
        <a href="https://www.comp.nus.edu.sg/~ayao/">Angela Yao</a> <br>
      <em> CVPR, 2025</em>
      <p>
        <a href="https://arxiv.org/pdf/2503.18359?">Paper</a>  <br><br> 
    </td>
</tr>


<!-- ITEM -->
<tr>
    <td style="padding:5px;width:40%;vertical-align:middle">
        <div style="text-align: center">
        <img src="images/mistake.jpg" alt="openegoAR" width="250" height="120" class="center">
        </div>
    </td>
    <td width="75%" valign="middle"><br>
        <a href="https://openreview.net/pdf?id=SNzpTSuuVJ">
        <papertitle>Spatial and temporal beliefs for mistake detection in assembly tasks </papertitle>
        </a> <br>
        <a href="https://guodongding.cn/">Guodong Ding</a>,
        <b>Fadime Sener*</b>,
        <a href="https://shugaoma.github.io/">Shugao Ma</a>,
        <a href="https://www.comp.nus.edu.sg/~ayao/">Angela Yao</a> <br>
      <em> CVIU, 2025</em>
      <p>
        <a href="https://openreview.net/pdf?id=SNzpTSuuVJ">Paper</a>  <br><br>
    </td>
</tr>


<!-- ITEM -->
<tr>
    <td style="padding:5px;width:40%;vertical-align:middle">
        <div style="text-align: center">
        <img src="images/tailTAS.jpg" alt="longtailTAS" width="250" height="120" class="center">
        </div>
    </td>
    <td width="75%" valign="middle"><br>
        <a href="https://arxiv.org/pdf/2408.09919">
        <papertitle>Long-Tail Temporal Action Segmentation with Group-wise Temporal Logit Adjustment</papertitle>
        </a> <br>
        <a href="https://pangzhan27.github.io/">Zhanzhong Pang</a>,
        <b>Fadime Sener*</b>,
        <a href="https://stablegradients.github.io/">Shrinivas Ramasubramanian</a>,
        <a href="https://www.comp.nus.edu.sg/~ayao/">Angela Yao</a> <br>
      <em>ECCV, 2024</em>
      <p>
        <a href="https://arxiv.org/pdf/2408.09919">Paper</a>  |
        <a href="https://github.com/pangzhan27/GTLA">Code</a>   <br><br>
    </td>
</tr>


<!-- ITEM -->
<tr>
    <td style="padding:5px;width:40%;vertical-align:middle">
        <div style="text-align: center">
        <img src="images/handformer.png" alt="handformer" width="250" height="120" class="center">
        </div>
    </td>
    <td width="75%" valign="middle"><br>
        <a href="https://arxiv.org/pdf/2403.09805">
        <papertitle>On the Utility of 3D Hand Poses for Action Recognition</papertitle>
        </a> <br>
        <a href="https://www.comp.nus.edu.sg/~salman/">Md Salman Shamil</a>,
        <a href="https://dibschat.github.io/">Dibyadip Chatterjee</a>,
        <b>Fadime Sener*</b>,
        <a href="https://shugaoma.github.io/">Shugao Ma</a>,
        <a href="https://www.comp.nus.edu.sg/~ayao/">Angela Yao</a> <br>
      <em>ECCV, 2024</em>
      <p>
        <a href="https://arxiv.org/pdf/2403.09805">Paper</a>  |
        <a href="https://github.com/s-shamil/HandFormer">Code</a> |
        <a href="https://s-shamil.github.io/HandFormer/">Project</a> <br><br>
    </td>
</tr>


<!-- ITEM -->
<tr>
    <td style="padding:5px;width:40%;vertical-align:middle">
        <div style="text-align: center">
        <img src="images/diffh2o.jpg" alt="DiffH2O" width="220" height="150" class="center">
        </div>
    </td>
    <td width="75%" valign="middle"><br>
        <a href="https://arxiv.org/pdf/2403.17827">
        <papertitle>DiffH2O: Diffusion-Based Synthesis of Hand-Object Interactions from Textual Descriptions</papertitle>
        </a> <br>
        <a href="https://ait.ethz.ch/people/sammyc">Sammy Christen</a>,
        <a href="https://shreyashampali.github.io/">Shreyas Hampali</a>,
        <b>Fadime Sener*</b>,
        <a href="https://scholar.google.com/citations?user=yz2P_aUAAAAJ&hl=en">Edoardo Remelli</a>,
        <a href="https://cmp.felk.cvut.cz/~hodanto2/">Tomas Hodan</a>,
        <a href="https://scholar.google.com/citations?user=HDYUYvwAAAAJ&hl=en">Eric Sauser </a>,
        <a href="https://shugaoma.github.io/">Shugao Ma</a><br>
        <a href="https://btekin.github.io/">Bugra Tekin</a>,
      <em>SIGGRAPH Asia, 2024</em>
      <p>
        <a href="https://arxiv.org/pdf/2403.17827">Paper</a>  |
        <a href="https://diffh2o.github.io/">Project</a> <br><br>
    </td>
</tr>


<!-- ITEM -->
<tr>
  <td style="padding:5px;width:40%;vertical-align:middle">
      <div style="text-align: center">
      <img src="images/xmic.png" alt="XMIC" width="220" height="150" class="center">
      </div>
  </td>
  <td width="75%" valign="middle"><br>
      <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Kukleva_X-MIC_Cross-Modal_Instance_Conditioning_for_Egocentric_Action_Generalization_CVPR_2024_paper.pdf">
      <papertitle>X-MIC: Cross-Modal Instance Conditioning for Egocentric Action Generalization</papertitle>
      </a> <br>
      <a href="https://annusha.github.io/">Anna Kukleva</a>,
      <b>Fadime Sener*</b>,
      <a href="https://scholar.google.com/citations?user=yz2P_aUAAAAJ&hl=en">Edoardo Remelli</a>,
      <a href="https://btekin.github.io/">Bugra Tekin</a>,
      <a href="https://scholar.google.com/citations?user=HDYUYvwAAAAJ&hl=en">Eric Sauser </a>,
      <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/people/bernt-schiele">Bernt Schiele</a>,
      <a href="https://shugaoma.github.io/">Shugao Ma</a><br>
    <em>CVPR, 2024</em>
    <p>
      <a href="https://arxiv.org/pdf/2403.19811v1.pdf">Paper</a>  |
      <a href="https://github.com/Annusha/xmic">Code</a> <br><br>
  </td>
</tr>


<!-- ITEM -->
<tr>
    <td style="padding:5px;width:40%;vertical-align:middle">
        <div style="text-align: center">
        <img src="images/openegoAR.png" alt="openegoAR" width="250" height="120" class="center">
        </div>
    </td>
    <td width="75%" valign="middle"><br>
        <a href="https://arxiv.org/pdf/2308.11488">
        <papertitle>Opening the Vocabulary of Egocentric Actions </papertitle>
        </a> <br>
        <a href="https://dibschat.github.io/">Dibyadip Chatterjee</a>,
        <b>Fadime Sener*</b>,
        <a href="https://shugaoma.github.io/">Shugao Ma</a>,
        <a href="https://www.comp.nus.edu.sg/~ayao/">Angela Yao</a> <br>
      <em>NeurIPS, 2023</em>
      <p>
        <a href="https://arxiv.org/pdf/2308.11488">Paper</a>  |
        <a href="https://github.com/dibschat/openvocab-egoAR">Code</a> |
        <a href="https://dibschat.github.io/openvocab-egoAR/">Project</a> <br><br>
    </td>
</tr>


<!-- ITEM -->
<tr>
    <td style="padding:5px;width:40%;vertical-align:middle">
        <div style="text-align: center">
        <img src="images/taskTAS.png" alt="openegoAR" width="250" height="120" class="center">
        </div>
    </td>
    <td width="75%" valign="middle"><br>
        <a href="https://arxiv.org/pdf/2210.10352">
        <papertitle>Temporal Action Segmentation: An Analysis of Modern Techniques </papertitle>
        </a> <br>
        <a href="https://guodongding.cn/">Guodong Ding</a>,
        <b>Fadime Sener*</b>,
        <a href="https://www.comp.nus.edu.sg/~ayao/">Angela Yao</a> <br>
      <em>TPAMI, 2023</em>
      <p>
        <a href="https://arxiv.org/pdf/2210.10352">Paper</a>  |
        <a href="https://github.com/nus-cvml/awesome-temporal-action-segmentation">Project</a> <br><br>
    </td>
</tr>




<!-- ITEM -->
<tr>
    <td style="padding:5px;width:40%;vertical-align:middle">
        <div style="text-align: center">
        <img src="images/assemblyhands.png" alt="openegoAR" width="210" height="160" class="center">
        </div>
    </td>
    <td width="75%" valign="middle"><br>
        <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ohkawa_AssemblyHands_Towards_Egocentric_Activity_Understanding_via_3D_Hand_Pose_Estimation_CVPR_2023_paper.pdf">
        <papertitle>AssemblyHands: Towards Egocentric Activity Understanding via 3D Hand Pose Estimation </papertitle>
        </a> <br>
        <a href="https://tkhkaeio.github.io/">Takehiko Ohkawa</a>,
        <a href="https://scholar.google.com/citations?user=eLFhiSYAAAAJ&hl=en">Kun He</a>,
        <b>Fadime Sener*</b>,
        <a href="https://cmp.felk.cvut.cz/~hodanto2/">Tomas Hodan</a>,
        <a href="https://scholar.google.co.jp/citations?user=YV0TVrQAAAAJ&hl">Luan Tran</a>,
        <a href="https://scholar.google.com/citations?user=9HoiYnYAAAAJ&hl=en">Cem Keskin</a> <br>
      <em>CVPR, 2023</em>
      <p>
        <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ohkawa_AssemblyHands_Towards_Egocentric_Activity_Understanding_via_3D_Hand_Pose_Estimation_CVPR_2023_paper.pdf">Paper</a>  |
        <a href="https://github.com/facebookresearch/assemblyhands-toolkit">Code</a> |
        <a href="https://assemblyhands.github.io/">Project</a> <br><br>
    </td>
</tr>


<!-- ITEM -->
<tr>
    <td style="padding:5px;width:40%;vertical-align:middle">
        <div style="text-align: center">
        <img src="images/zeroshotTpami.jpg" alt="openegoAR" width="250" height="120" class="center">
        </div>
    </td>
    <td width="75%" valign="middle"><br>
        <a href="https://arxiv.org/pdf/2106.03158">
        <papertitle>Transferring Knowledge from Text to Video: Zero-Shot Anticipation for Procedural Actions </papertitle>
        </a> <br>
        <b>Fadime Sener*</b>,
        <a href="https://scholar.google.co.in/citations?user=HRHD7_EAAAAJ&hl=en"> Rishabh Saraf</a>,
        <a href="https://www.comp.nus.edu.sg/~ayao/">Angela Yao</a> <br>
      <em>TPAMI, 2022</em>
      <p>
        <a href="https://arxiv.org/pdf/2106.03158">Paper</a>
    </td>
</tr>


<!-- ITEM -->
<tr>
    <td style="padding:5px;width:40%;vertical-align:middle">
        <div style="text-align: center">
        <img src="images/troi.jpg" alt="openegoAR" width="250" height="120" class="center">
        </div>
    </td>
    <td width="75%" valign="middle"><br>
        <a href="https://arxiv.org/pdf/2106.03162">
        <papertitle>Transformed ROIs for Capturing Visual Transformations in Videos </papertitle>
        </a> <br>
        <a href="https://abhinavrai44.github.io/"> Abhinav Rai</a>,
        <b>Fadime Sener*</b>,
        <a href="https://www.comp.nus.edu.sg/~ayao/">Angela Yao</a> <br>
      <em>CVIU, 2022</em>
      <p>
        <a href="https://arxiv.org/pdf/2106.03162">Paper</a>
    </td>
</tr>


<!-- ITEM -->
<tr>
    <td style="padding:5px;width:40%;vertical-align:middle">
        <div style="text-align: center">
        <img src="images/assembly.png" alt="openegoAR" width="250" height="120" class="center">
        </div>
    </td>
    <td width="75%" valign="middle"><br>
        <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Sener_Assembly101_A_Large-Scale_Multi-View_Video_Dataset_for_Understanding_Procedural_Activities_CVPR_2022_paper.pdf">
        <papertitle>Assembly101: A Large-Scale Multi-View Video Dataset for Understanding Procedural Activities </papertitle>
        </a> <br>
        <b>Fadime Sener*</b>,
        <a href="https://dibschat.github.io/">Dibyadip Chatterjee</a>,
        <a href="">Daniel Shelepov</a>,
        <a href="">Kun He</a>,
        <a href="">Dipika Singhania</a>,
        <a href="">Robert Wang</a>,
        <a href="https://www.comp.nus.edu.sg/~ayao/">Angela Yao</a> <br>
      <em>CVPR, 2022</em><br>
      <em> <a href="https://egovis.github.io/awards/2022_2023/"> EgoVis 2022/2023 Distinguished Paper Winner</em>
      <p>
        <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Sener_Assembly101_A_Large-Scale_Multi-View_Video_Dataset_for_Understanding_Procedural_Activities_CVPR_2022_paper.pdf">Paper</a>  |
        <a href="https://github.com/assembly-101?tab=repositories">Code</a> |
        <a href="https://assembly-101.github.io/">Project</a> |
        <a href="https://www.youtube.com/watch?v=8T8x-iWa3qw">Video</a> <br><br>
    </td>
</tr>


<!-- ITEM -->
<tr>
    <td style="padding:5px;width:40%;vertical-align:middle">
        <div style="text-align: center">
        <img src="images/tempagg_tech.jpg" alt="openegoAR" width="250" height="120" class="center">
        </div>
    </td>
    <td width="75%" valign="middle"><br>
        <a href="https://arxiv.org/pdf/2106.03152">
        <papertitle>Technical report: Temporal aggregate representations</papertitle>
        </a> <br>
        <b>Fadime Sener*</b>,
        <a href="https://dibschat.github.io/">Dibyadip Chatterjee</a>,
        <a href="https://www.comp.nus.edu.sg/~ayao/">Angela Yao</a> <br>
      <em>Arxiv, EPIC-KITCHENS-Challenges, 2021</em>
      <p>
        <a href="https://arxiv.org/pdf/2106.03152">Paper</a>  |
        <a href="https://github.com/dibschat/tempAgg">Code</a>
    </td>
</tr>


<!-- ITEM -->
<tr>
    <td style="padding:5px;width:40%;vertical-align:middle">
        <div style="text-align: center">
        <img src="images/tempagg.jpg" alt="openegoAR" width="250" height="120" class="center">
        </div>
    </td>
    <td width="75%" valign="middle"><br>
        <a href="https://arxiv.org/pdf/2006.00830">
        <papertitle>Temporal aggregate representations for long-range video understanding</papertitle>
        </a> <br>
        <b>Fadime Sener*</b>,
        <a href="">Dipika Singhania</a>,
        <a href="https://www.comp.nus.edu.sg/~ayao/">Angela Yao</a> <br>
      <em>ECCV, 2020</em>
      <p>
        <a href="https://arxiv.org/pdf/2006.00830">Paper</a>  |
        <a href="https://github.com/dibschat/tempAgg">Code</a>
    </td>
</tr>


<!-- ITEM -->
<tr>
    <td style="padding:5px;width:40%;vertical-align:middle">
      <div style="text-align: center">
      <img src="images/cvpr19.png" alt="cet" width="240" height="100" class="center">
      </div>
    </td>
    <td width="75%" valign="middle"><br>
      <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Kukleva_Unsupervised_Learning_of_Action_Classes_With_Continuous_Temporal_Embedding_CVPR_2019_paper.pdf">
      <papertitle>Unsupervised Learning of Action Classes with Continuous Temporal Embedding</papertitle>
      </a><br>
          <a href="https://annusha.github.io/">Anna Kukleva</a>,
          <a href="https://hildekuehne.github.io/">Hilde Kuehne</a>,
          <b>Fadime Sener*</b>,
          <a href="https://pages.iai.uni-bonn.de/gall_juergen/">J√ºrgen Gall</a> <br>
        <em>CVPR, 2019</em>
        <p>
          <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Kukleva_Unsupervised_Learning_of_Action_Classes_With_Continuous_Temporal_Embedding_CVPR_2019_paper.pdf">Paper</a> |
          <a href="https://github.com/Annusha/unsup_temp_embed">Code</a> <br><br>
    </td>
</tr>


<!-- ITEM -->
<tr>
  <td style="padding:5px;width:40%;vertical-align:middle">
      <div style="text-align: center">
      <img src="images/furniture.jpg" alt="openegoAR" width="210" height="120" class="center">
      </div>
  </td>
  <td width="75%" valign="middle"><br>
      <a href="https://arxiv.org/pdf/1812.03570">
      <papertitle>Learning Style Compatibility for Furniture</papertitle>
      </a> <br>
      <a href="https://divyanshaggarwal.github.io/">Divyansh Aggarwal</a>,
      <a href="https://de.linkedin.com/in/elchin-valiyev-114995b5">Elchin Valiyev</a>,
      <b>Fadime Sener*</b>,
      <a href="https://www.comp.nus.edu.sg/~ayao/">Angela Yao</a> <br>
    <em>GCPR, 2019</em>
    <p>
      <a href="https://arxiv.org/pdf/1812.03570">Paper</a>
  </td>
</tr>


<!-- ITEM -->
<tr>
  <td style="padding:5px;width:40%;vertical-align:middle">
      <div style="text-align: center">
      <img src="images/zeroshot.jpg" alt="openegoAR" width="270" height="50" class="center">
      </div>
  </td>
  <td width="75%" valign="middle"><br>
      <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Sener_Zero-Shot_Anticipation_for_Instructional_Activities_ICCV_2019_paper.pdf">
      <papertitle>Zero-Shot Anticipation for Instructional Activities</papertitle>
      </a> <br>
      <b>Fadime Sener*</b>,
      <a href="https://www.comp.nus.edu.sg/~ayao/">Angela Yao</a> <br>
    <em>ICCV, 2019</em>
    <p>
      <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Sener_Zero-Shot_Anticipation_for_Instructional_Activities_ICCV_2019_paper.pdf">Paper</a>
  </td>
</tr>


<!-- ITEM -->
<tr>
  <td style="padding:5px;width:40%;vertical-align:middle">
      <div style="text-align: center">
      <img src="images/unsupervised.jpg" alt="openegoAR" width="210" height="120" class="center">
      </div>
  </td>
  <td width="75%" valign="middle"><br>
      <a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Sener_Unsupervised_Learning_and_CVPR_2018_paper.pdf">
      <papertitle>Unsupervised Learning and Segmentation of Complex Activities from Video</papertitle>
      </a> <br>
      <b>Fadime Sener*</b>,
      <a href="https://www.comp.nus.edu.sg/~ayao/">Angela Yao</a> <br>
    <em>CVPR, 2018. Spotlight</em>
    <p>
      <a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Sener_Unsupervised_Learning_and_CVPR_2018_paper.pdf">Paper</a>
  </td>
</tr>


<!-- ITEM -->
<tr>
  <td style="padding:5px;width:40%;vertical-align:middle">
      <div style="text-align: center">
      <img src="images/draw.jpg" alt="openegoAR" width="160" height="120" class="center">
      </div>
    </td>
    <td width="75%" valign="middle"><br>
        <a href="https://arxiv.org/pdf/1704.03057">
        <papertitle>DRAW: Deep networks for Recognizing styles of Artists Who illustrate children's books</papertitle>
        </a> <br>
        <a href="">Samet Hicsonmez</a>,
        <a href="">Nermin Samet</a>,
        <b>Fadime Sener*</b>,
        <a href="https://scholar.google.co.uk/citations?user=1KEMrHkAAAAJ&hl=en/">Pinar Duygulu</a> <br>
      <em>ACM ICMR, 2017</em>
      <p>
        <a href="https://arxiv.org/pdf/1704.03057">Paper</a>
    </td>
  </tr>


<!-- ITEM -->
<tr>
  <td style="padding:5px;width:40%;vertical-align:middle">
      <div style="text-align: center">
      <img src="images/twopersonint.jpg" alt="openegoAR" width="210" height="120" class="center">
      </div>
  </td>
  <td width="75%" valign="middle"><br>
      <a href="https://www.sciencedirect.com/science/article/pii/S104732031500142X">
      <papertitle>Two-person interaction recognition via spatial multiple instance embedding</papertitle>
      </a> <br>
      <b>Fadime Sener*</b>,
      <a href="https://web.cs.hacettepe.edu.tr/~nazli//">Nazli Ikizler-Cinbis</a> <br>
    <em>JVCIR, 2015</em>
    <p>
      <a href="https://www.sciencedirect.com/science/article/pii/S104732031500142X">Paper</a>
  </td>
</tr>


<!-- ITEM -->
<tr>
  <td style="padding:5px;width:40%;vertical-align:middle">
      <div style="text-align: center">
      <img src="images/ensemble_image.jpg" alt="openegoAR" width="210" height="120" class="center">
      </div>
  </td>
  <td width="75%" valign="middle"><br>
      <a href="https://www.sciencedirect.com/science/article/pii/S0262885614000511">
      <papertitle>Ensemble of multiple instance classifiers for image re-ranking</papertitle>
      </a> <br>
      <b>Fadime Sener*</b>,
      <a href="https://web.cs.hacettepe.edu.tr/~nazli//">Nazli Ikizler-Cinbis</a> <br>
    <em>IMAVIS, 2014</em>
    <p>
      <a href="https://www.sciencedirect.com/science/article/pii/S0262885614000511">Paper</a>
  </td>
</tr>


<!-- ITEM -->
<tr>
  <td style="padding:5px;width:40%;vertical-align:middle">
      <div style="text-align: center">
      <img src="images/eccvChildren.jpg" alt="openegoAR" width="210" height="120" class="center">
      </div>
  </td>
  <td width="75%" valign="middle"><br>
      <a href="https://link.springer.com/chapter/10.1007/978-3-642-33863-2_61">
      <papertitle>Identification of illustrators</papertitle>
      </a> <br>
      <b>Fadime Sener*</b>,
      <a href="">Nermin Samet</a>,
      <a href="https://scholar.google.co.uk/citations?user=1KEMrHkAAAAJ&hl=en/">Pinar Duygulu</a> <br>
    <em>ECCV'W, 2012</em>
    <p>
      <a href="https://link.springer.com/chapter/10.1007/978-3-642-33863-2_61">Paper</a>
  </td>
</tr>


<!-- ITEM -->
<tr>
  <td style="padding:5px;width:40%;vertical-align:middle">
      <div style="text-align: center">
      <img src="images/mileccv.jpg" alt="openegoAR" width="170" height="120" class="center">
      </div>
  </td>
  <td width="75%" valign="middle"><br>
      <a href="https://link.springer.com/chapter/10.1007/978-3-642-33885-4_27">
      <papertitle>On recognizing actions in still images via multiple features</papertitle>
      </a> <br>
      <b>Fadime Sener*</b>,
      <a href="https://web.cs.hacettepe.edu.tr/~nazli//">Nazli Ikizler-Cinbis</a> <br>
      <em>ECCV'W, 2012</em>
    <p>
      <a href="https://link.springer.com/chapter/10.1007/978-3-642-33885-4_27">Paper</a>
  </td>
</tr>






</tbody>
</table>
<!--Publication  End ---------------------------------------------->


<!--Ref Start ---------------------------------------------->
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
<tbody>
  <tr>
  <td style="padding:0px"> <br>
  <p style="text-align:right;font-size:small;"> <br>
  Webpage template from <a href="https://jonbarron.info/">Jon Barron</a>. Big thanks!
  </p>
  </td>
  </tr>
</tbody>
</table>
<!--Ref End ---------------------------------------------->


</td>
</tr>
</table>
</body>
</html>
